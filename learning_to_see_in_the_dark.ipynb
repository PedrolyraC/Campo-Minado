{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14SIDFJOOyxc7iIihcqUtw_7a4ePEIlAZ",
      "authorship_tag": "ABX9TyNxojg2ZGyDTMw9ShvC4kjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedrolyraC/Campo-Minado/blob/main/learning_to_see_in_the_dark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8sxBKclRjnH",
        "outputId": "fafec3c6-fb1d-40f4-8572-022119253a5e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia -smi"
      ],
      "metadata": {
        "id": "b-xHCRlrssqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "import tensorflow.keras.callbacks as tfkc\n",
        "import tensorflow.keras.initializers as tfki\n",
        "import tensorflow.keras.layers as tfkl\n",
        "import tensorflow.keras.models as tfkm\n",
        "import tensorflow.keras.preprocessing as tfkp\n",
        "import tensorflow.keras.optimizers as tfko\n",
        "import tensorflow.keras.utils as tfku\n",
        "\n",
        "from collections import OrderedDict\n",
        "from einops import rearrange\n",
        "from tensorflow.keras import Sequential\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "hkW1_PVF6bxR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "YAML_PATH = '/content/drive/MyDrive/Computer Vision/options.yml'\n",
        "LOL_PATH = '/content/drive/MyDrive/Computer Vision/Dataset/train_datasets/lol_dataset'\n",
        "SID_PATH = '/content/drive/MyDrive/Computer Vision/Dataset/train_datasets/SID dataset'"
      ],
      "metadata": {
        "id": "-DxGhm4i5Vm5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PSNR(Peak Signal-to-Noise Ratio)**"
      ],
      "metadata": {
        "id": "5iLgZ8fKfLil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PSNR(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='psnr', dtype=tf.float32, **kwargs):\n",
        "        super(PSNR, self).__init__(name=name, dtype=dtype, **kwargs)\n",
        "        self.psnr_sum = self.add_weight(name='psnr_sum', initializer='zeros')\n",
        "        self.total_samples = self.add_weight(name='total_samples', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        psnr = tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "        psnr = tf.cast(psnr, self.dtype)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "            sample_weight = tf.broadcast_to(sample_weight, psnr.shape)\n",
        "            psnr = tf.multiply(psnr, sample_weight)\n",
        "\n",
        "        self.psnr_sum.assign_add(tf.reduce_sum(psnr))\n",
        "        self.total_samples.assign_add(tf.cast(tf.size(psnr), self.dtype))\n",
        "\n",
        "    def result(self):\n",
        "        return self.psnr_sum / self.total_samples if self.total_samples != 0.0 else 0.0\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.psnr_sum.assign(0)\n",
        "        self.total_samples.assign(0)"
      ],
      "metadata": {
        "id": "ijgHyQDA6m5I"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataloaders**"
      ],
      "metadata": {
        "id": "b9DJuKBYxGIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseDataLoader:\n",
        "    def __init__(self, root_dir, validation_split, seed, train):\n",
        "        self.root_dir = root_dir\n",
        "        self.validation_split = validation_split if train else 0.0\n",
        "        self.seed = seed\n",
        "        self.train = train\n",
        "\n",
        "    def load_dataset(self, dataset, image_size):\n",
        "        full_dir = os.path.join(\n",
        "            self.root_dir, dataset, 'train' if self.train else 'test'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            lq_ds, gt_ds = tfkp.image_dataset_from_directory(\n",
        "                full_dir,\n",
        "                labels=None,\n",
        "                color_mode='rgb',\n",
        "                batch_size=None,\n",
        "                image_size=image_size,\n",
        "                shuffle=False,\n",
        "                seed=self.seed,\n",
        "                validation_split=0.5,\n",
        "                subset='both',\n",
        "                crop_to_aspect_ratio=True,\n",
        "            )\n",
        "        except:\n",
        "            print(f'No dataset found')\n",
        "            pass\n",
        "\n",
        "        return lq_ds, gt_ds"
      ],
      "metadata": {
        "id": "LmOLKd_DCr50"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RawDataLoader(BaseDataLoader):\n",
        "    def load_dataset(self, dataset, image_size):\n",
        "        lq_ds, lq_val_ds, _, _ = super().load_dataset(dataset, image_size)\n",
        "        full_dir = os.path.join(\n",
        "            self.root_dir, dataset, 'train' if self.train else 'test'\n",
        "        )\n",
        "\n",
        "        all_images = []\n",
        "        for img_path in tqdm(lq_ds.file_paths + lq_val_ds.file_paths):\n",
        "            img_id = os.path.basename(img_path).split('_')[0]\n",
        "            try:\n",
        "                img = tfku.load_img(\n",
        "                    os.path.join(full_dir, 'target', f'{img_id}_00_10s.png'),\n",
        "                    target_size = image_size\n",
        "                )\n",
        "            except FileNotFoundError:\n",
        "                img = tfku.load_img(\n",
        "                    os.path.join(full_dir, 'target', f'{img_id}_00_30s.png'),\n",
        "                    target_size = image_size\n",
        "                )\n",
        "\n",
        "            img_array = tfku.img_to_array(img)\n",
        "            all_images.append(img_array)\n",
        "\n",
        "        all_images_np = np.array(all_images)\n",
        "        num_val_samples = int(self.validation_split * len(all_images_np))\n",
        "        gt_ds = tf.data.Dataset.from_tensor_slices(all_images_np[:-num_val_samples])\n",
        "        gt_val_ds = tf.data.Dataset.from_tensor_slices(all_images_np[-num_val_samples:])\n",
        "\n",
        "        return lq_ds, lq_val_ds, gt_ds, gt_val_ds"
      ],
      "metadata": {
        "id": "K6pCVr5u4RR0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(\n",
        "    root_dir,\n",
        "    dataset,\n",
        "    image_size,\n",
        "    validation_split=0.2,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    train=True\n",
        "  ):\n",
        "\n",
        "    loader = BaseDataLoader(root_dir, validation_split, seed, train)\n",
        "    lq_ds, gt_ds = loader.load_dataset(dataset, image_size)\n",
        "\n",
        "    lq_fp = lq_ds.file_paths\n",
        "    gt_fp = gt_ds.file_paths\n",
        "\n",
        "    assert lq_fp == [\n",
        "        path.replace('target', 'input').replace('-gt', '') for path in gt_fp\n",
        "    ], f'Mismatach in dataset alignment for {dataset}: lq {lq_fp} != gt {gt_fp}'\n",
        "\n",
        "    train_ds = tf.data.Dataset.zip(lq_ds, gt_ds)\n",
        "\n",
        "    if train:\n",
        "        buffer_div = lq_ds.element_spec.shape[1] / 640\n",
        "\n",
        "        data_size = train_ds.cardinality()\n",
        "        train_size = round(data_size.numpy() * (1 - validation_split))\n",
        "        buffer_size = min(data_size.numpy(), data_size.numpy() // buffer_div)\n",
        "\n",
        "        if shuffle:\n",
        "            print(f'Shuffling: {dataset} {str(image_size)} dataset')\n",
        "            train_ds = train_ds.shuffle(\n",
        "                  buffer_size=buffer_size,\n",
        "                  seed=seed,\n",
        "                  reshuffle_each_iteration=True\n",
        "            )\n",
        "\n",
        "        val_ds = train_ds.skip(train_size)\n",
        "        train_ds = train_ds.take(train_size)\n",
        "\n",
        "        return train_ds, val_ds\n",
        "    else:\n",
        "        return train_ds, None\n",
        "\n",
        "def prepare_train_dataset(\n",
        "    root_dir,\n",
        "    save_dir,\n",
        "    save_ds,\n",
        "    data_dirs,\n",
        "    batch_size,\n",
        "    validation_split=0.2,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    augment=True\n",
        "  ):\n",
        "    train_save_dir = os.path.join(save_dir, 'train')\n",
        "    val_save_dir = os.path.join(save_dir, 'val')\n",
        "\n",
        "    if os.path.exists(f'{train_save_dir}/ALL') and os.path.exist(\n",
        "        f'{val_save_dir}/ALL'\n",
        "    ):\n",
        "        print(f'loading cached datasets from {train_save_dir}/ALL')\n",
        "        train_ds = tf.data.Dataset.load(f'{train_save_dir}/ALL', compression='GZIP')\n",
        "        val_ds = tf.data.Dataset.load(f'{val_save_dir}/ALL', compression='GZIP')\n",
        "    else:\n",
        "        for dataset, image_size in data_dirs.items():\n",
        "            print(f'processing: {dataset} {str(image_size)} dataset')\n",
        "            dataset_train_path = os.path.join(train_save_dir, dataset, str(image_size))\n",
        "            dataset_val_path = os.path.join(val_save_dir, dataset, str(image_size))\n",
        "\n",
        "            if os.path.exists(dataset_train_path) and os.path.exists(dataset_val_path):\n",
        "                temp_train_ds = tf.data.Dataset.load(dataset_train_path, compression='GZIP')\n",
        "                temp_val_ds = tf.data.Dataset.load(dataset_val_path, compression='GZIP')\n",
        "            else:\n",
        "                temp_train_ds, temp_val_ds = load_datasets(\n",
        "                    root_dir,\n",
        "                    dataset,\n",
        "                    image_size,\n",
        "                    validation_split=validation_split,\n",
        "                    seed=seed,\n",
        "                    shuffle=shuffle,\n",
        "                    train=True\n",
        "                )\n",
        "                print(f'Batching {dataset} {str(image_size)} dataset')\n",
        "                temp_train_ds = temp_train_ds.batch(batch_size['train'])\n",
        "                temp_val_ds = temp_val_ds.batch(batch_size['val'])\n",
        "\n",
        "                if augment:\n",
        "                    print(f'augmenting: {dataset} {str(image_size)} dataset')\n",
        "                    temp_train_ds = apply_augmentation(temp_train_ds, seed=seed)\n",
        "\n",
        "                if save_ds:\n",
        "                    print(f'Saving: {dataset} {str(image_size)} dataset')\n",
        "                    temp_train_ds.save(dataset_train_path, compression='GZIP')\n",
        "                    temp_val_ds.save(dataset_val_path, compression='GZIP')\n",
        "\n",
        "            if  'train_ds' in locals():\n",
        "                train_ds = train_ds.concatenate(temp_train_ds)\n",
        "            else:\n",
        "                print(f'Concatenating dataset: {dataset} {str(image_size)}')\n",
        "                train_ds = temp_train_ds\n",
        "\n",
        "            if 'val_ds' in locals():\n",
        "              val_ds = val_ds.concatenate(temp_val_ds)\n",
        "            else:\n",
        "              val_ds = temp_val_ds\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def prepare_test_dataset(root_dir, save_dir, save_ds, data_dirs, batch_size, seed=None):\n",
        "    test_save_dir = os.path.join(save_dir, 'test')\n",
        "    if os.path.exists(f'{test_save_dir}/ALL'):\n",
        "        print(f'Loading cached datasets from {test_save_dir}/ALL')\n",
        "        test_ds = tf.data.Dataset.load(f'{test_save_dir}/ALL', compression='GZIP')\n",
        "    else:\n",
        "        for dataset, image_size in data_dirs.items():\n",
        "            dataset_test_path = os.path.join(save_dir, 'test', dataset, str(image_size))\n",
        "\n",
        "            if os.path.exists(dataset_test_path):\n",
        "                temp_test_ds = tf.data.Dataset.load(\n",
        "                    dataset_test_path, compression='GZIP'\n",
        "                )\n",
        "            else:\n",
        "                temp_test_ds, _ = load_datasets(\n",
        "                    root_dir,\n",
        "                    dataset,\n",
        "                    image_size,\n",
        "                    validation_split=0,\n",
        "                    seed=seed,\n",
        "                    train=False,\n",
        "                )\n",
        "                print(f'Batching: {dataset} {str(image_size)} dataset')\n",
        "                temp_test_ds = temp_test_ds.batch(batch_size['val'])\n",
        "\n",
        "                if save_ds:\n",
        "                    print(f'Saving: {dataset} {str(image_size)} dataset')\n",
        "                    temp_test_ds.save(dataset_test_path, compression='GZIP')\n",
        "\n",
        "            if 'test_ds' in locals():\n",
        "                print(f'Concatenating dataset: {dataset} {str(image_size)}')\n",
        "                test_ds = test_ds.concatenate(temp_test_ds)\n",
        "            else:\n",
        "                test_ds = temp_test_ds\n",
        "        if save_ds:\n",
        "            test_ds.save(os.path.join(save_dir, 'val/ALL'), compression='GZIP')\n",
        "\n",
        "    return test_ds\n",
        "\n",
        "def prepare_predict_dataset(directory):\n",
        "    size = None\n",
        "\n",
        "    for file in os.listdir(directory):\n",
        "        full_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(full_path):\n",
        "            try:\n",
        "                with Image.open(full_path) as img:\n",
        "                    size = img.size\n",
        "                    break\n",
        "            except IOError:\n",
        "                pass\n",
        "\n",
        "    if size is None:\n",
        "        raise ValueError('No valid images found in the directory.')\n",
        "\n",
        "    dataset = tfkp.image_dataset_from_directory(\n",
        "        directory,\n",
        "        labels=None,\n",
        "        color_mode='rgb',\n",
        "        batch_size=None,\n",
        "        image_size=size,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, options, seed=None):\n",
        "        print('Instantiating DataLoader...')\n",
        "        self.root_dir = options['root_dir']\n",
        "        self.save_dir = options['save_dir']\n",
        "        self.data_dirs = options['data_dirs']\n",
        "        self.validation_split = options['validation_split']\n",
        "        self.batch_size = options['batch_size']\n",
        "        self.use_augment = options['use_augment']\n",
        "        self.use_shuffle = options['use_shuffle']\n",
        "        self.save_ds = options['save_ds']\n",
        "        self.seed = seed\n",
        "\n",
        "    def load_train_data(self):\n",
        "        train_ds, val_ds = prepare_train_dataset(\n",
        "            self.root_dir,\n",
        "            self.save_dir,\n",
        "            self.save_ds,\n",
        "            self.data_dirs,\n",
        "            self.batch_size,\n",
        "            self.validation_split,\n",
        "            self.seed,\n",
        "            self.use_shuffle,\n",
        "            self.use_augment,\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            train_ds.prefetch(tf.data.AUTOTUNE),\n",
        "            val_ds.prefetch(tf.data.AUTOTUNE),\n",
        "        )\n",
        "\n",
        "    def load_test_data(self):\n",
        "        test_ds = prepare_test_dataset(\n",
        "            self.root_dir,\n",
        "            self.save_dir,\n",
        "            self.save_ds,\n",
        "            self.data_dirs,\n",
        "            self.batch_size,\n",
        "            self.seed,\n",
        "        )\n",
        "\n",
        "        return test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    def load_predict_data(self, predict_dir):\n",
        "        return prepare_predict_dataset(predict_dir)"
      ],
      "metadata": {
        "id": "Eh359TdiD_CS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation**"
      ],
      "metadata": {
        "id": "Mt9X6qFdw67m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairedImageAugumentation(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PairedImageAugumentation, self).__init__()\n",
        "        self.h_flip = tf.image.flip_left_right\n",
        "        self.v_flip = tf.image.flip_up_down\n",
        "\n",
        "    def call(self, inputs, seed=None):\n",
        "        lq_img_batch, gt_img_batch = inputs\n",
        "\n",
        "        h_flip_seed = tf.random.uniform([], seed=seed, minval=0, maxval=2, dtype=tf.int32)\n",
        "        v_flip_seed = tf.random.uniform([], seed=seed, minval=0, maxval=2, dtype=tf.int32)\n",
        "        rot_flip_seed = tf.random.uniform([], seed=seed, minval=0, maxval=4, dtype=tf.int32)\n",
        "\n",
        "        lq_img_batch = tf.cond(h_flip_seed == 1, lambda: self.h_flip(lq_img_batch), lambda: lq_img_batch)\n",
        "        gt_img_batch = tf.cond(h_flip_seed == 1, lambda: self.h_flip(gt_img_batch), lambda: gt_img_batch)\n",
        "\n",
        "        lq_img_batch = tf.cond(v_flip_seed == 1, lambda: self.v_flip(lq_img_batch), lambda: lq_img_batch)\n",
        "        gt_img_batch = tf.cond(v_flip_seed == 1, lambda: self.v_flip(gt_img_batch), lambda: gt_img_batch)\n",
        "\n",
        "        lq_img_batch = tf.image.rot90(lq_img_batch, k=rot_flip_seed)\n",
        "        gt_img_batch = tf.image.rot90(gt_img_batch, k=rot_flip_seed)\n",
        "\n",
        "        return lq_img_batch, gt_img_batch\n",
        "\n",
        "def apply_augmentation(dataset, seed=None):\n",
        "  custom_augmentation = PairedImageAugumentation()\n",
        "\n",
        "  def data_augmentation(lq_img_batch, gt_img_batch, seed=seed):\n",
        "      normalized_lq_batch = lq_img_batch/255.0\n",
        "      normalized_gt_batch = gt_img_batch/255.0\n",
        "\n",
        "      augmented_lq_batch, augmented_gt_batch = custom_augmentation(\n",
        "          lq_img_batch, gt_img_batch, seed=seed\n",
        "      )\n",
        "      return augmented_lq_batch, augmented_gt_batch\n",
        "\n",
        "  return dataset.map(\n",
        "      lambda lq_img_batch, gt_img_batch: data_augmentation(\n",
        "          lq_img_batch, gt_img_batch, seed=seed\n",
        "      ),\n",
        "      num_parallel_calls=AUTO\n",
        "  )"
      ],
      "metadata": {
        "id": "z92uNxCTEhfD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "Z-CWjweo8plz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineDecayCycleRestarts(tfko.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_lr,\n",
        "        first_decay_steps,\n",
        "        t_mul=2.0,\n",
        "        m_mul=1.0,\n",
        "        alpha=[0.0, 0.0],\n",
        "        name=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.first_decay_steps = first_decay_steps\n",
        "        self._t_mul = t_mul\n",
        "        self._m_mul = m_mul\n",
        "        self.alpha = alpha\n",
        "        self.name = name\n",
        "\n",
        "    def __call__(self, step):\n",
        "        with tf.name_scope(self.name or 'SGDRDecay') as name:\n",
        "            base_lr = tf.convert_to_tensor(self.base_lr, name='base_lr')\n",
        "            dtype = base_lr.dtype\n",
        "            first_decay_steps = tf.cast(self.first_decay_steps, dtype)\n",
        "            t_mul = tf.cast(self._t_mul, dtype)\n",
        "            m_mul = tf.cast(self._m_mul, dtype)\n",
        "\n",
        "            global_step_recomp = tf.cast(step, dtype)\n",
        "            completed_fraction = global_step_recomp / first_decay_steps\n",
        "\n",
        "            alpha = tf.cond(\n",
        "                tf.greater(completed_fraction, 1.0),\n",
        "                lambda: tf.cast(self.alpha[1], dtype),\n",
        "                lambda: tf.cast(self.alpha[0], dtype),\n",
        "            )\n",
        "\n",
        "            def compute_step(completed_fraction, geometric=False):\n",
        "                if geometric:\n",
        "                    i_restart = tf.floor(\n",
        "                        tf.math.log(1.0 - completed_fraction * (1.0 - t_mul))\n",
        "                        / tf.math.log(t_mul)\n",
        "                    )\n",
        "\n",
        "                    sum_r = (1.0 - t_mul**i_restart) / (1.0 - t_mul)\n",
        "                    completed_fraction = (\n",
        "                        completed_fraction - sum_r\n",
        "                    ) / t_mul**i_restart\n",
        "\n",
        "                else:\n",
        "                    i_restart = tf.floor(completed_fraction)\n",
        "                    completed_fraction -= i_restart\n",
        "\n",
        "                return i_restart, completed_fraction\n",
        "\n",
        "            i_restart, completed_fraction = tf.cond(\n",
        "                tf.equal(t_mul, 1.0),\n",
        "                lambda: compute_step(completed_fraction, geometric=False),\n",
        "                lambda: compute_step(completed_fraction, geometric=True),\n",
        "            )\n",
        "\n",
        "            m_fac = m_mul**i_restart\n",
        "            cosine_decayed = (\n",
        "                0.5\n",
        "                * m_fac\n",
        "                * (1.0 + tf.cos(tf.constant(math.pi, dtype=dtype) * completed_fraction))\n",
        "            )\n",
        "            decayed = (1 - alpha) * cosine_decayed + alpha\n",
        "\n",
        "            return tf.multiply(base_lr, decayed, name=name)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'base_lr': self.base_lr,\n",
        "            'first_decay_steps': self.first_decay_steps,\n",
        "            't_mul': self._t_mul,\n",
        "            'm_mul': self._m_mul,\n",
        "            'alpha': self.alpha,\n",
        "            'name': self.name,\n",
        "        }"
      ],
      "metadata": {
        "id": "h61w29pp8s9F"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dim_swap(tensor, order=[]):\n",
        "    if len(order) != 2:\n",
        "        raise ValueError('Order list must have exactly two elements.')\n",
        "\n",
        "    ndims = len(tensor.shape)\n",
        "\n",
        "    order = [d if d >= 0 else ndims + d for d in order]\n",
        "\n",
        "    if order[0] >= ndims or order[1] >= ndims or order[0] < 0 or order[1] < 0:\n",
        "        raise IndexError('Order indices are out of range for the tensor dimensions.')\n",
        "\n",
        "    perm = list(range(ndims))\n",
        "    perm[order[0]], perm[order[1]] = perm[order[1]], perm[order[0]]\n",
        "\n",
        "    return tf.transpose(tensor, perm=perm)\n",
        "\n",
        "\n",
        "def flatten(input_tensor, start_dim, end_dim):\n",
        "    shape = tf.shape(input_tensor)\n",
        "    slice_numel = tf.reduce_prod(shape[start_dim : end_dim + 1])\n",
        "    new_shape = tf.concat(\n",
        "        [\n",
        "            shape[:start_dim],\n",
        "            [slice_numel],\n",
        "            shape[end_dim + 1 :],\n",
        "        ],\n",
        "        axis=0,\n",
        "    )\n",
        "\n",
        "    return tf.reshape(input_tensor, new_shape)\n",
        "\n",
        "class PreNorm(tfkl.Layer):\n",
        "    def __init__(self, fn):\n",
        "        super(PreNorm, self).__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = tfkl.LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "class Illumination_Estimator(tfkl.Layer):\n",
        "    def __init__(self, n_fea_middle, n_fea_out=3):\n",
        "        super(Illumination_Estimator, self).__init__()\n",
        "\n",
        "        self.conv1 = tfkl.Conv2D(n_fea_middle, kernel_size=1, use_bias=True)\n",
        "        self.depth_conv = tfkl.DepthwiseConv2D(\n",
        "            kernel_size=5, padding='same', use_bias=True\n",
        "        )\n",
        "\n",
        "        self.conv2 = tfkl.Conv2D(n_fea_out, kernel_size=1, use_bias=True)\n",
        "\n",
        "    def call(self, img):\n",
        "        mean_c = tf.expand_dims(tf.reduce_mean(img, axis=3), axis=3)\n",
        "        input = tf.concat([img, mean_c], axis=3)\n",
        "\n",
        "        x_1 = self.conv1(input)\n",
        "        illu_fea = self.depth_conv(x_1)\n",
        "        illu_map = self.conv2(illu_fea)\n",
        "        return illu_fea, illu_map\n",
        "\n",
        "\n",
        "class IG_MSA(tfkm.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head=40,\n",
        "        heads=8,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.to_q = tfkl.Dense(\n",
        "            dim_head * heads,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=tfki.TruncatedNormal(stddev=0.02),\n",
        "        )\n",
        "        self.to_k = tfkl.Dense(\n",
        "            dim_head * heads,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=tfki.TruncatedNormal(stddev=0.02),\n",
        "        )\n",
        "        self.to_v = tfkl.Dense(\n",
        "            dim_head * heads,\n",
        "            use_bias=False,\n",
        "            kernel_initializer=tfki.TruncatedNormal(stddev=0.02),\n",
        "        )\n",
        "        self.rescale = tf.Variable(tf.ones([heads, 1, 1]))\n",
        "        self.proj = tfkl.Dense(\n",
        "            dim, use_bias=True, kernel_initializer=tfki.TruncatedNormal(stddev=0.02)\n",
        "        )\n",
        "        self.pos_emb = Sequential(\n",
        "            [\n",
        "                tfkl.DepthwiseConv2D(\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    use_bias=True,\n",
        "                    activation='gelu',\n",
        "                ),\n",
        "                tfkl.DepthwiseConv2D(kernel_size=3, padding='same', use_bias=True),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dim = dim\n",
        "\n",
        "    def call(self, x_in, illu_fea_trans):\n",
        "        b, h, w, c = (\n",
        "            tf.shape(x_in)[0],\n",
        "            tf.shape(x_in)[1],\n",
        "            tf.shape(x_in)[2],\n",
        "            tf.shape(x_in)[3],\n",
        "        )\n",
        "        x = tf.reshape(x_in, [b, h * w, c])\n",
        "        q_inp = self.to_q(x)\n",
        "        k_inp = self.to_k(x)\n",
        "        v_inp = self.to_v(x)\n",
        "        illu_attn = illu_fea_trans\n",
        "        illu_attn_flat = flatten(illu_attn, 1, 2)\n",
        "        q, k, v, illu_attn = map(\n",
        "            lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads),\n",
        "            (q_inp, k_inp, v_inp, illu_attn_flat),\n",
        "        )\n",
        "        v = v * illu_attn\n",
        "        q = dim_swap(q, [-2, -1])\n",
        "        k = dim_swap(k, [-2, -1])\n",
        "        v = dim_swap(v, [-2, -1])\n",
        "        q = tf.nn.l2_normalize(q)\n",
        "        k = tf.nn.l2_normalize(k)\n",
        "        attn = tf.matmul(k, dim_swap(q, [-2, -1]))\n",
        "        attn = attn * self.rescale\n",
        "        attn = tf.nn.softmax(attn)\n",
        "        x = tf.matmul(attn, v)\n",
        "        x = tf.transpose(x, [0, 3, 1, 2])\n",
        "        x = tf.reshape(x, [b, h * w, self.num_heads * self.dim_head])\n",
        "        out_c = tf.reshape(self.proj(x), [b, h, w, c])\n",
        "        out_p = self.pos_emb(tf.reshape(v_inp, [b, h, w, c]))\n",
        "        out = out_c + out_p\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForward(tfkm.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = Sequential(\n",
        "            [\n",
        "                tfkl.DepthwiseConv2D(\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    use_bias=False,\n",
        "                    activation='gelu',\n",
        "                ),\n",
        "                tfkl.DepthwiseConv2D(\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    use_bias=False,\n",
        "                    activation='gelu',\n",
        "                ),\n",
        "                tfkl.DepthwiseConv2D(kernel_size=1, strides=1, use_bias=False),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.net(x)\n",
        "        return out\n",
        "\n",
        "class IGAB(tfkl.Layer):\n",
        "    def __init__(self, dim, dim_head=40, heads=8, num_blocks=2):\n",
        "        super().__init__()\n",
        "        self.blocks = []\n",
        "        for _ in range(num_blocks):\n",
        "            self.blocks.append(\n",
        "                [\n",
        "                    IG_MSA(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                    PreNorm(fn=FeedForward()),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    def call(self, x, illu_fea):\n",
        "        for attn, ff in self.blocks:\n",
        "            x = attn(x, illu_fea_trans=illu_fea) + x\n",
        "            x = ff(x) + x\n",
        "        out = x\n",
        "        return out\n",
        "\n",
        "class Corruption_Restorer(tfkl.Layer):\n",
        "    def __init__(self, out_dim=3, dim=40, level=2, num_blocks=[1, 2, 2]):\n",
        "        super(Corruption_Restorer, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.level = level\n",
        "\n",
        "        # Input projection\n",
        "        self.embedding = tfkl.Conv2D(\n",
        "            self.dim, kernel_size=3, strides=1, padding='same', use_bias=False\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layers = []\n",
        "        dim_level = dim\n",
        "        for i in range(level):\n",
        "            self.encoder_layers.append(\n",
        "                [\n",
        "                    IGAB(\n",
        "                        dim=dim_level,\n",
        "                        num_blocks=num_blocks[i],\n",
        "                        dim_head=dim,\n",
        "                        heads=dim_level // dim,\n",
        "                    ),\n",
        "                    tfkl.Conv2D(\n",
        "                        dim_level * 2,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        use_bias=False,\n",
        "                    ),\n",
        "                    tfkl.Conv2D(\n",
        "                        dim_level * 2,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        use_bias=False,\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "            dim_level *= 2\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = IGAB(\n",
        "            dim=dim_level,\n",
        "            dim_head=dim,\n",
        "            heads=dim_level // dim,\n",
        "            num_blocks=num_blocks[-1],\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_layers = []\n",
        "        for i in range(level):\n",
        "            self.decoder_layers.append(\n",
        "                [\n",
        "                    tfkl.Conv2DTranspose(dim_level // 2, kernel_size=2, strides=2),\n",
        "                    tfkl.Conv2D(\n",
        "                        dim_level // 2, kernel_size=1, strides=1, use_bias=False\n",
        "                    ),\n",
        "                    IGAB(\n",
        "                        dim=dim_level // 2,\n",
        "                        num_blocks=num_blocks[level - 1 - i],\n",
        "                        dim_head=dim,\n",
        "                        heads=(dim_level // 2) // dim,\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "            dim_level //= 2\n",
        "\n",
        "        # Output projection\n",
        "        self.mapping = tfkl.Conv2D(\n",
        "            out_dim, kernel_size=3, strides=1, padding='same', use_bias=False\n",
        "        )\n",
        "\n",
        "    def call(self, x, illu_fea):\n",
        "        # Embedding\n",
        "        fea = self.embedding(x)\n",
        "\n",
        "        # Encoder\n",
        "        fea_encoder = []\n",
        "        illu_fea_list = []\n",
        "        for IGAB, FeaDownSample, IlluFeaDownsample in self.encoder_layers:\n",
        "            fea = IGAB(fea, illu_fea)\n",
        "            illu_fea_list.append(illu_fea)\n",
        "            fea_encoder.append(fea)\n",
        "            fea = FeaDownSample(fea)\n",
        "            illu_fea = IlluFeaDownsample(illu_fea)\n",
        "\n",
        "        # Bottleneck\n",
        "        fea = self.bottleneck(fea, illu_fea)\n",
        "\n",
        "        # Decoder\n",
        "        for i, (FeaUpSample, Fution, LeWinBlcok) in enumerate(self.decoder_layers):\n",
        "            fea = FeaUpSample(fea)\n",
        "            fea = Fution(\n",
        "                tfkl.concatenate([fea, fea_encoder[self.level - 1 - i]], axis=-1)\n",
        "            )\n",
        "            illu_fea = illu_fea_list[self.level - 1 - i]\n",
        "            fea = LeWinBlcok(fea, illu_fea)\n",
        "\n",
        "        # Mapping\n",
        "        out = self.mapping(fea) + x\n",
        "        return out\n",
        "\n",
        "class RetinexFormer_Single_Stage(tfkl.Layer):\n",
        "    def __init__(self, out_channels=3, n_feat=40, level=2, num_blocks=[1, 2, 2]):\n",
        "        super(RetinexFormer_Single_Stage, self).__init__()\n",
        "        self.estimator = Illumination_Estimator(n_feat)\n",
        "        self.denoiser = Corruption_Restorer(\n",
        "            out_dim=out_channels,\n",
        "            dim=n_feat,\n",
        "            level=level,\n",
        "            num_blocks=num_blocks,\n",
        "        )\n",
        "\n",
        "    def call(self, img):\n",
        "        illu_fea, illu_map = self.estimator(img)\n",
        "        input_img = img * illu_map + img\n",
        "        output_img = self.denoiser(input_img, illu_fea)\n",
        "\n",
        "        return output_img\n",
        "\n",
        "class RetinexFormer(tfkm.Model):\n",
        "    def __init__(self, out_channels=3, n_feat=40, stage=1, num_blocks=[1, 2, 2]):\n",
        "        super(RetinexFormer, self).__init__()\n",
        "        self.stage = stage\n",
        "\n",
        "        self.body = Sequential(\n",
        "            [\n",
        "                RetinexFormer_Single_Stage(\n",
        "                    out_channels=out_channels,\n",
        "                    n_feat=n_feat,\n",
        "                    level=2,\n",
        "                    num_blocks=num_blocks,\n",
        "                )\n",
        "                for _ in range(stage)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.body(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "PrYHYKol6pw4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrintLR(tfkc.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = self.model.optimizer._decayed_lr(tf.float32).numpy()\n",
        "        print(f'Learning rate for epoch {epoch + 1} is {lr:.6f}')\n",
        "\n",
        "class RetinexFormerModel:\n",
        "    def __init__(self, options):\n",
        "        self.options = options\n",
        "        self.seed = self.options['manual_seed']\n",
        "        self.checkpoint_dir = (\n",
        "            self.options['checkpoint_dir']\n",
        "            if 'checkpoint_dir' in self.options\n",
        "            else './model/training_checkpoints'\n",
        "        )\n",
        "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, 'ckpt_{epoch:04d}')\n",
        "        self.initial_epoch = 0\n",
        "        self.logs_dir = self.options['logs_dir']\n",
        "        self.dataset_options = self.options['dataset']\n",
        "        self.model_options = self.options['model']\n",
        "        self.training_options = self.options['training']\n",
        "        self.data_loader = DataLoader(self.dataset_options, self.seed)\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        tfku.set_random_seed(self.seed)\n",
        "        model = RetinexFormer(**self.model_options)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        first_decay_steps = (\n",
        "            self.training_options['scheduler']['periods'][0]\n",
        "            // self.dataset_options['batch_size']\n",
        "        )\n",
        "        base_lr = self.training_options['optimizer']['lr']\n",
        "        t_mul = (\n",
        "            self.training_options['scheduler']['periods'][1]\n",
        "            / self.training_options['scheduler']['periods'][0]\n",
        "        )\n",
        "        m_mul = self.training_options['scheduler']['m_mul']\n",
        "        alpha = [\n",
        "            alpha / base_lr for alpha in self.training_options['scheduler']['lr_mins']\n",
        "        ]\n",
        "        clipnorm = 0.01 if self.training_options['optimizer']['clipnorm'] else None\n",
        "        beta_1 = self.training_options['optimizer']['betas'][0]\n",
        "        beta_2 = self.training_options['optimizer']['betas'][1]\n",
        "\n",
        "        learning_rate_schedule = CosineDecayCycleRestarts(\n",
        "            first_decay_steps=first_decay_steps,\n",
        "            base_lr=base_lr,\n",
        "            t_mul=t_mul,\n",
        "            m_mul=m_mul,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "        self.model.compile(\n",
        "            optimizer=tfko.Adam(\n",
        "                learning_rate=learning_rate_schedule,\n",
        "                beta_1=beta_1,\n",
        "                beta_2=beta_2,\n",
        "                global_clipnorm=clipnorm,\n",
        "            ),\n",
        "            loss='mae',\n",
        "            metrics=[f'accuracy {PSNR()}'],\n",
        "        )\n",
        "\n",
        "    def load_weights(self):\n",
        "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
        "\n",
        "        if latest_checkpoint:\n",
        "            print(f'Loading weights from {latest_checkpoint}')\n",
        "            self.model.load_weights(latest_checkpoint)\n",
        "            checkpoint_name = os.path.basename(latest_checkpoint)\n",
        "            self.initial_epoch = int(checkpoint_name.split('_')[-1])\n",
        "\n",
        "    def train(self):\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        train_ds, val_ds = self.data_loader.load_train_data()\n",
        "        epochs = max(\n",
        "            1,\n",
        "            self.training_options['total_iter']\n",
        "            // (train_ds.cardinality().numpy() * self.dataset_options['batch_size']),\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            tfkc.ModelCheckpoint(\n",
        "                filepath=self.checkpoint_prefix, verbose=1, save_weights_only=True\n",
        "            ),\n",
        "            tfkc.TensorBoard(\n",
        "                log_dir=self.logs_dir, histogram_freq=1, profile_batch='500,520'\n",
        "            ),\n",
        "            tfkc.CSVLogger(os.path.join(self.logs_dir, 'training.log')),\n",
        "            PrintLR(),\n",
        "        ]\n",
        "\n",
        "        self.model.fit(\n",
        "            train_ds,\n",
        "            epochs=epochs,\n",
        "            initial_epoch=self.initial_epoch,\n",
        "            verbose='auto',\n",
        "            validation_data=val_ds,\n",
        "            shuffle=True,\n",
        "            callbacks=callbacks,\n",
        "        )\n",
        "\n",
        "    def evaluate(self):\n",
        "        test_ds = self.data_loader.load_test_data()\n",
        "\n",
        "        callbacks = [\n",
        "            tfkc.TensorBoard(log_dir=self.logs_dir, histogram_freq=1),\n",
        "            tfkc.CSVLogger(os.path.join(self.logs_dir, 'test.log')),\n",
        "        ]\n",
        "\n",
        "        return self.model.evaluate(\n",
        "            test_ds,\n",
        "            callbacks=callbacks,\n",
        "        )\n",
        "\n",
        "    def predict(self, data):\n",
        "        predict_data = self.data_loader.load_predict_data(data)\n",
        "        return self.model.predict(predict_data)"
      ],
      "metadata": {
        "id": "NuKb_wswyBxu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run**"
      ],
      "metadata": {
        "id": "0MgI_S4U9TNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ordered_yaml():\n",
        "    try:\n",
        "        from yaml import CDumper as Dumper\n",
        "        from yaml import CLoader as Loader\n",
        "    except ImportError:\n",
        "        from yaml import Dumper, Loader\n",
        "\n",
        "    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n",
        "\n",
        "    def dict_representer(dumper, data):\n",
        "        return dumper.represent_dict(data.items())\n",
        "\n",
        "    def dict_constructor(loader, node):\n",
        "        return OrderedDict(loader.construct_pairs(node))\n",
        "\n",
        "    Dumper.add_representer(OrderedDict, dict_representer)\n",
        "    Loader.add_constructor(_mapping_tag, dict_constructor)\n",
        "    return Loader, Dumper\n",
        "\n",
        "def parse_yaml(opt_path):\n",
        "    with open(opt_path, mode='r') as f:\n",
        "        Loader, _ = ordered_yaml()\n",
        "        opt = yaml.load(f, Loader=Loader)\n",
        "\n",
        "    return opt\n",
        "\n",
        "def parse_options():\n",
        "    parser = argparse.ArgumentParser(description='Run RetinexFormer model')\n",
        "    parser.add_argument(\n",
        "        '-m',\n",
        "        '--mode',\n",
        "        metavar='\\b',\n",
        "        choices=['train', 'test', 'predict'],\n",
        "        required=True,\n",
        "        help='Select a mode to run the model in [\"train\", \"test\", \"predict\"]',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '-o',\n",
        "        '--opt',\n",
        "        metavar='\\b',\n",
        "        type=str,\n",
        "        default=YAML_PATH,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '-e',\n",
        "        '--enhance',\n",
        "        metavar='\\b',\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help='Path to image file(s) to enhance',\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    with open(args.opt, 'r') as stream:\n",
        "        try:\n",
        "            opt = yaml.safe_load(stream)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "\n",
        "    for arg in vars(args):\n",
        "        opt[arg] = getattr(args, arg)\n",
        "\n",
        "    seed = opt.get('manual_seed')\n",
        "    if seed is None:\n",
        "        seed = random.randint(1, 10000)\n",
        "        opt['manual_seed'] = seed\n",
        "\n",
        "    return opt"
      ],
      "metadata": {
        "id": "9JX8MILAyNhS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options = parse_options()\n",
        "\n",
        "model = RetinexFormerModel(options)\n",
        "\n",
        "if options['mode'] == 'train':\n",
        "    model.compile_model()\n",
        "    model.load_weights()\n",
        "    model.train()\n",
        "elif options['mode'] == 'test':\n",
        "    model.load_weights()\n",
        "    model.evaluate()\n",
        "elif options['mode'] == 'predict':\n",
        "    model.load_weights()\n",
        "    model.predict(options.enhance)"
      ],
      "metadata": {
        "id": "c4fS0LkgYKSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b2eababd-0d7f-469d-ad9b-5e83bb70b409"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] -m \b [-o \b] [-e \b]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: -m/--mode\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}