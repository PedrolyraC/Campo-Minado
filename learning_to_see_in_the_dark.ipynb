{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2fECbu779lSRlZEVSTG0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedrolyraC/Campo-Minado/blob/main/learning_to_see_in_the_dark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PIL.Image as Image\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.callbacks as tfkc\n",
        "import tensroflow.keras.initializers as tkfi\n",
        "import tensorflow.keras.layers as tfkl\n",
        "import tensorflow.keras.optimizers as tfko\n",
        "import tensorflow.keras.preprocessing as tfkp\n",
        "import tensorflow.keras.utils as tfku\n",
        "\n",
        "from einops import rearrange\n",
        "from tensorflow.keras import Sequential"
      ],
      "metadata": {
        "id": "hkW1_PVF6bxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PSNR(Peak Signal-to-Noise Ratio)**"
      ],
      "metadata": {
        "id": "5iLgZ8fKfLil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PSNR(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='psnr', dtype=tf.float32, **kwargs):\n",
        "        super(PSNR, self).__init__(name=name, dtype=dtype, **kwargs)\n",
        "        self.psnr_sum = self.add_weight(name='psnr_sum', initializer='zeros')\n",
        "        self.total_samples = self.add_weight(name='total_samples', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        psnr = tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "        psnr = tf.cast(psnr, self.dtype)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "            sample_weight = tf.broadcast_to(sample_weight, psnr.shape)\n",
        "            psnr = tf.multiply(psnr, sample_weight)\n",
        "\n",
        "        self.psnr_sum.assign_add(tf.reduce_sum(psnr))\n",
        "        self.total_samples.assign_add(tf.cast(tf.size(psnr), self.dtype))\n",
        "\n",
        "    def result(self):\n",
        "        return self.psnr_sum / self.total_samples if self.total_samples != 0.0 else 0.0\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.psnr_sum.assign(0)\n",
        "        self.total_samples.assign(0)"
      ],
      "metadata": {
        "id": "ijgHyQDA6m5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataloaders**"
      ],
      "metadata": {
        "id": "b9DJuKBYxGIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseDataLoader:\n",
        "    def __init__(self, root_dir, validation_split, seed, train):\n",
        "        self.root_dir = root_dir\n",
        "        self.validation_split = validation_split if train else 0.0\n",
        "        self.seed = seed\n",
        "        self.train = train\n",
        "\n",
        "    def load_dataset(self, dataset, image_size):\n",
        "        full_dir = os.path.join(\n",
        "            self.root_dir, dataset, 'train' if self.train else 'test'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            lq_ds, gt_ds = tfkp.image_dataset_from_directory(\n",
        "                full_dir,\n",
        "                labels=None,\n",
        "                color_mode='rgb',\n",
        "                batch_size=None,\n",
        "                image_size=image_size,\n",
        "                shuffle=False,\n",
        "                seed=self.seed,\n",
        "                validation_split=0.5,\n",
        "                subset='both',\n",
        "                crop_to_aspect_ratio=True,\n",
        "            )\n",
        "        except:\n",
        "            print(f'No dataset found')\n",
        "            pass\n",
        "\n",
        "        return lq_ds, gt_ds"
      ],
      "metadata": {
        "id": "LmOLKd_DCr50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RawDataLoader(BaseDataLoader):\n",
        "    def load_dataset(self, dataset, image_size):\n",
        "        lq_ds, lq_val_ds, -, - = super().load_dataset(dataset, image_size)\n",
        "        full_dir = os.path.join(\n",
        "            self.root_dir, dataset, 'train', if self.train else 'test'\n",
        "        )\n",
        "\n",
        "        all_images = []\n",
        "        for imag_path in tqdm(lq_ds.file_paths + lq_val_ds.file_paths):\n",
        "            img_id = os.path.basename(img_path).split('_')[0]\n",
        "            try:\n",
        "                img = tfku.load_img(\n",
        "                    os.path.join(full_dir, 'target', f'{img_id}_00_10s.png'),\n",
        "                    target_size = image_size\n",
        "                )\n",
        "            except FileNotFoundError:\n",
        "                img = tfku.load_img(\n",
        "                    os.path.join(full_dir, 'target', f'{img_id}_00_30s.png'),\n",
        "                    target_size = image_size\n",
        "                )\n",
        "\n",
        "            img_array = tfku.img_to_array(img)\n",
        "            all_images.append(img_array)\n",
        "\n",
        "        all_images_np = np.array(all_images)\n",
        "        num_val_samples = int(self.validation_split * len(all_images_np))\n",
        "        gt_ds = tf.data.Dataset.from_tensor_slices(all_images_np[:-num_val_samples])\n",
        "        gt_val_ds = tf.data.Dataset.from_tensor_slices(all_images_np[-num_val_samples:])\n",
        "\n",
        "        return lq_ds, lq_val_ds, gt_ds, gt_val_ds"
      ],
      "metadata": {
        "id": "K6pCVr5u4RR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(\n",
        "    root_dir,\n",
        "    dataset,\n",
        "    image_size,\n",
        "    validation_split=0.2,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    train=True\n",
        "  ):\n",
        "\n",
        "    loader = BaseDataLoader(root_dir, validation_split, seed, train)\n",
        "    lq_ds, gt_ds = loader.load_dataset(dataset, image_size)\n",
        "\n",
        "    lq_fp = lq_ds.file_paths\n",
        "    gt_fp = gt_ds.file_paths\n",
        "\n",
        "    assert lq_fp == [\n",
        "        path.replace('target', 'input').replace('-gt', '') for path in gt_fp\n",
        "    ], f'Mismatach in dataset alignment for {dataset}: lq {lq_fp} != gt {gt_fp}'\n",
        "\n",
        "    train_ds = tf.data.Dataset.zip(lq_ds, gt_ds)\n",
        "\n",
        "    if train:\n",
        "        buffer_div = lq_ds.element_spec.shape[1] / 640\n",
        "\n",
        "        data_size = train_ds.cardinality()\n",
        "        train_size = round(data_size.numpy() * (1 - validation_split))\n",
        "        buffer_size = min(data_size.numpy(), data_size.numpy() // buffer_div)\n",
        "\n",
        "        if shuffle:\n",
        "            print(f'Shuffling: {dataset} {str(image_size)} dataset')\n",
        "            train_ds = train_ds.shuffle(\n",
        "                  buffer_size=buffer_size,\n",
        "                  seed=seed,\n",
        "                  reshuffle_each_iteration=True\n",
        "            )\n",
        "\n",
        "        val_ds = train_ds.skip(train_size)\n",
        "        train_ds = train_ds.take(train_size)\n",
        "\n",
        "        return train_ds, val_ds\n",
        "    else:\n",
        "        return train_ds, None\n",
        "\n",
        "def prepare_train_dataset(\n",
        "    root_dir,\n",
        "    save_dir,\n",
        "    save_ds,\n",
        "    date_dirs,\n",
        "    batch_size,\n",
        "    validation_split=0.2,\n",
        "    seed=None,\n",
        "    shuffle=True,\n",
        "    augment=True\n",
        "  ):\n",
        "    train_save_dir = os.path.join(save_dir, 'train')\n",
        "    val_save_dir = os.path.join(save_dir, 'val')\n",
        "\n",
        "    if os.path.exists(f'{train_save_dir}/ALL') and os.path.exist(\n",
        "        f'{val_save_dir}/ALL'\n",
        "    ):\n",
        "        print(f'loading cached datasets from {train_save_dir}/ALL')\n",
        "        train_ds = tf.data.Dataset.load(f'{train_save_dir}/ALL', compression='GZIP')\n",
        "        val_ds = tf.data.Dataset.load(f'{val_save_dir}/ALL', compression='GZIP')\n",
        "    else:\n",
        "        for dataset, image_size in data_dirs.items():\n",
        "            print(f'processing: {dataset} {str(image_size)} dataset')\n",
        "            dataset_train_path = os.path.join(train_save_dir, dataset, str(image_size))\n",
        "            dataset_val_path = os.path.join(val_save_dir, dataset, str(image_size))\n",
        "\n",
        "            if os.path.exists(dataset_train_path) and os.path.exists(dataset_val_path):\n",
        "                temp_train_ds = tf.data.Dataset.load(dataset_train_path, compression='GZIP')\n",
        "                temp_val_ds = tf.data.Dataset.load(dataset_val_path, compression='GZIP')\n",
        "            else:\n",
        "                temp_train_ds, temp_val_ds = load_datasets(\n",
        "                    root_dir,\n",
        "                    dataset,\n",
        "                    image_size,\n",
        "                    validation_split=validation_split,\n",
        "                    seed=seed,\n",
        "                    shuffle=shuffle,\n",
        "                    train=True\n",
        "                )\n",
        "                print(f'Batching {dataset} {str(image_size)} dataset')\n",
        "                temp_train_ds = temp_train_ds.batch(batch_size['train'])\n",
        "                temp_val_ds = temp_val_ds.batch(batch_size['val'])\n",
        "\n",
        "                if augment:\n",
        "                    print(f'augmenting: {dataset} {str(image_size)} dataset')\n",
        "                    temp_train_ds = apply_augmentation(temp_train_ds, seed=seed)\n",
        "\n",
        "                if save_ds:\n",
        "                    print(f'Saving: {dataset} {str(image_size)} dataset')\n",
        "                    temp_train_ds.save(dataset_train_path, compression='GZIP')\n",
        "                    temp_val_ds.save(dataset_val_path, compression='GZIP')\n",
        "\n",
        "            if  'train_ds' in locals():\n",
        "                train_ds = train_ds.concatenate(temp_train_ds)\n",
        "            else:\n",
        "                print(f'Concatenating dataset: {dataset} {str(image_size)}')\n",
        "                train_ds = temp_train_ds\n",
        "\n",
        "            if 'val_ds' in locals():\n",
        "              val_ds = val_ds.concatenate(temp_val_ds)\n",
        "            else:\n",
        "              val_ds = temp_val_ds\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def prepare_test_dataset(root_dir, save_dir, save_ds, data_dirs, batch_size, seed=None):\n",
        "    test_save_dir = os.path.join(save_dir, 'test')\n",
        "    if os.path.exists(f\"{test_save_dir}/ALL\"):\n",
        "        print(f'Loading cached datasets from {test_save_dir}/ALL')\n",
        "        test_ds = tf.data.Dataset.load(f'{test_save_dir}/ALL', compression='GZIP')\n",
        "    else:\n",
        "        for dataset, image_size in data_dirs.items():\n",
        "            dataset_test_path = os.path.join(save_dir, 'test', dataset, str(image_size))\n",
        "\n",
        "            if os.path.exists(dataset_test_path):\n",
        "                temp_test_ds = tf.data.Dataset.load(\n",
        "                    dataset_test_path, compression='GZIP'\n",
        "                )\n",
        "            else:\n",
        "                temp_test_ds, _ = load_datasets(\n",
        "                    root_dir,\n",
        "                    dataset,\n",
        "                    image_size,\n",
        "                    validation_split=0,\n",
        "                    seed=seed,\n",
        "                    train=False,\n",
        "                )\n",
        "                print(f'Batching: {dataset} {str(image_size)} dataset')\n",
        "                temp_test_ds = temp_test_ds.batch(batch_size['val'])\n",
        "\n",
        "                if save_ds:\n",
        "                    print(f'Saving: {dataset} {str(image_size)} dataset')\n",
        "                    temp_test_ds.save(dataset_test_path, compression='GZIP')\n",
        "\n",
        "            if \"test_ds\" in locals():\n",
        "                print(f'Concatenating dataset: {dataset} {str(image_size)}')\n",
        "                test_ds = test_ds.concatenate(temp_test_ds)\n",
        "            else:\n",
        "                test_ds = temp_test_ds\n",
        "        if save_ds:\n",
        "            test_ds.save(os.path.join(save_dir, 'val/ALL'), compression='GZIP')\n",
        "\n",
        "    return test_ds\n",
        "\n",
        "def prepare_predict_dataset(directory):\n",
        "    size = None\n",
        "\n",
        "    for file in os.listdir(directory):\n",
        "        full_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(full_path):\n",
        "            try:\n",
        "                with Image.open(full_path) as img:\n",
        "                    size = img.size\n",
        "                    break\n",
        "            except IOError:\n",
        "                pass\n",
        "\n",
        "    if size is None:\n",
        "        raise ValueError('No valid images found in the directory.')\n",
        "\n",
        "    dataset = tfkp.image_dataset_from_directory(\n",
        "        directory,\n",
        "        labels=None,\n",
        "        color_mode='rgb',\n",
        "        batch_size=None,\n",
        "        image_size=size,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, options, seed=None):\n",
        "        print('Instantiating DataLoader...')\n",
        "        self.root_dir = options['root_dir']\n",
        "        self.save_dir = options['save_dir']\n",
        "        self.data_dirs = options['data_dirs']\n",
        "        self.validation_split = options['validation_split']\n",
        "        self.batch_size = options['batch_size']\n",
        "        self.use_augment = options['use_augment']\n",
        "        self.use_shuffle = options['use_shuffle']\n",
        "        self.save_ds = options['save_ds']\n",
        "        self.seed = seed\n",
        "\n",
        "    def load_train_data(self):\n",
        "        train_ds, val_ds = prepare_train_dataset(\n",
        "            self.root_dir,\n",
        "            self.save_dir,\n",
        "            self.save_ds,\n",
        "            self.data_dirs,\n",
        "            self.batch_size,\n",
        "            self.validation_split,\n",
        "            self.seed,\n",
        "            self.use_shuffle,\n",
        "            self.use_augment,\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            train_ds.prefetch(tf.data.AUTOTUNE),\n",
        "            val_ds.prefetch(tf.data.AUTOTUNE),\n",
        "        )\n",
        "\n",
        "    def load_test_data(self):\n",
        "        test_ds = prepare_test_dataset(\n",
        "            self.root_dir,\n",
        "            self.save_dir,\n",
        "            self.save_ds,\n",
        "            self.data_dirs,\n",
        "            self.batch_size,\n",
        "            self.seed,\n",
        "        )\n",
        "\n",
        "        return test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    def load_predict_data(self, predict_dir):\n",
        "        return prepare_predict_dataset(predict_dir)"
      ],
      "metadata": {
        "id": "Eh359TdiD_CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation**"
      ],
      "metadata": {
        "id": "Mt9X6qFdw67m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairedImageAugumentation(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PairedImageAugumentation, self).__init__()\n",
        "        self.h_flip = tf.image.flip_left_right\n",
        "        self.v_flip = tf.image.flip_up_down\n",
        "\n",
        "    def call(self, inputs, seed=None):\n",
        "        lq_img_batch, gt_img_batch = inputs\n",
        "\n",
        "        h_flip_seed = tf.random.uniform([], seed=seed, minval=0, maxval=2, dtype=tf.int32)\n",
        "        v_flip_seed = tf.random.uniform([], seed=seed, minval=0, maxval=2, dtype=tf.int32)\n",
        "        rot_flip_seed = tf.random.uniform([], seed=seed, minval=0, maxval=4, dtype=tf.int32)\n",
        "\n",
        "        lq_img_batch = tf.cond(h_flip_seed == 1, lambda: self.h_flip(lq_img_batch), lambda: lq_img_batch)\n",
        "        gt_img_batch = tf.cond(h_flip_seed == 1, lambda: self.h_flip(gt_img_batch), lambda: gt_img_batch)\n",
        "\n",
        "        lq_img_batch = tf.cond(v_flip_seed == 1, lambda: self.v_flip(lq_img_batch), lambda: lq_img_batch)\n",
        "        gt_img_batch = tf.cond(v_flip_seed == 1, lambda: self.v_flip(gt_img_batch), lambda: gt_img_batch)\n",
        "\n",
        "        lq_img_batch = tf.image.rot90(lq_imq_batch, k=rot_flip_seed)\n",
        "        gt_img_batch = tf.image.rot90(gt_img_batch, k=rot_flip_seed)\n",
        "\n",
        "        return lq_img_batch, gt_img_batch\n",
        "\n",
        "    def apply_augmentation(dataset, seed=None):\n",
        "        custom_augmentation = PairedImageAugmentation()\n",
        "\n",
        "        def data_augmentation(lq_img_batch, gt_img_batch, seed=seed):\n",
        "            normalized_lq_batch = lq_img_batch/255.0\n",
        "            normalized_gt_batch = gt_img_batch/255.0\n",
        "\n",
        "            augmented_lq_batch, augmented_gt_batch = custom_augmentation(\n",
        "                lq_img_batch, gt_img_batch, seed=seed\n",
        "            )\n",
        "            return augmented_lq_batch, augmented_gt_batch\n",
        "\n",
        "        return dataset.map(\n",
        "            lambda lq_img_batch, gt_img_batch: data_augmentation(\n",
        "                lq_img_batch, gt_img_batch, seed=seed\n",
        "            ),\n",
        "            num_parallel_calls=AUTO\n",
        "        )"
      ],
      "metadata": {
        "id": "z92uNxCTEhfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "Z-CWjweo8plz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h61w29pp8s9F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}